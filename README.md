# Capstone project for Airflow GidU course

## Infrastructure
Infrastructure can be created with docker-compose (airflow DB initialization service is required to be run first and then can be commented out again).
Relative paths to be created before containers startup:
- `./compose_data/postgresql`
- `./compose_data/redis`
- `./compose_data/airflow`

Following files/directories have to be put into `./compose_data/airflow`:
- `airflow.cfg`
- `dags/`
- `plugins/`

## Example DB setup
As tasks interact with DB it must first be cretaed. The steps are the following:
```
# entering running postgreSQL container
sudo docker-compose exec postgres /bin/bash
```
```
# inside container
psql airflow airflow
create database gridu;
create user gridu with encrypted password 'gridu';
grant all privileges on database gridu to gridu;
```

## Trigger file creation
As one of the tasks in monotoring presence of file in local filesystem, the file should be created on specific worker node.
The worker is always the same as it serves dedicated queue (see queue definition in docker-compose).
```
sudo docker-compose exec airflow-worker-1 /bin/bash -c 'touch /tmp/trigger.txt'
```

## Course-related DAGs
Although there are several DAGs in the repo they are not all related to the course tasks.
The ones to check are:
- `dags/jobs_DAG.py` - generates 3 almost identical DAGs differentiating only by parameters
- `dags/trigger_DAG.py` - triggers one of DAGs generated by `jobs_DAG.py` and motitors its status by subDAG defined in the same file.
